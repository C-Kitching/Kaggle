{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports and config","metadata":{}},{"cell_type":"code","source":"# Essentials\nimport numpy as np\nimport scipy\nimport pandas as pd\nimport datetime\nimport random\nfrom scipy import stats\nfrom scipy.stats import kstest\nfrom scipy.stats import boxcox\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Models\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import Ridge, RidgeCV, LassoCV, ElasticNet, ElasticNetCV\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\n# Stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n# Misc\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\n\npd.set_option('display.max_columns', None)\n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\npd.options.display.max_seq_items = 8000\npd.options.display.max_rows = 8000","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:37:40.414889Z","iopub.execute_input":"2023-01-19T14:37:40.415336Z","iopub.status.idle":"2023-01-19T14:37:40.429744Z","shell.execute_reply.started":"2023-01-19T14:37:40.415297Z","shell.execute_reply":"2023-01-19T14:37:40.428509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Date loading","metadata":{}},{"cell_type":"code","source":"def read_data():\n    \"\"\"\n    Read in train and test data\n    \n    Args:\n        None\n        \n    Returns:\n        train (pandas dataframe) : train data\n        test (pandas dataframe) : test data\n    \"\"\"\n    \n    # Read in the dataset as a dataframe\n    train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\n    test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n    \n    return train, test\n\ntrain, test = read_data()\ntrain.shape, test.shape","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:37:40.431707Z","iopub.execute_input":"2023-01-19T14:37:40.432071Z","iopub.status.idle":"2023-01-19T14:37:40.494562Z","shell.execute_reply.started":"2023-01-19T14:37:40.432036Z","shell.execute_reply":"2023-01-19T14:37:40.493056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"Let's first preview the data.","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:37:40.496642Z","iopub.execute_input":"2023-01-19T14:37:40.497152Z","iopub.status.idle":"2023-01-19T14:37:40.555262Z","shell.execute_reply.started":"2023-01-19T14:37:40.497116Z","shell.execute_reply":"2023-01-19T14:37:40.553945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SalePrice : the target","metadata":{}},{"cell_type":"markdown","source":"The saleprice is what we are trying to predict, so let's have a look at a plot of this.","metadata":{}},{"cell_type":"code","source":"sns.set_style(\"white\")\nsns.set_color_codes(palette = 'deep')\nfigure, ax = plt.subplots(figsize = (8, 7))\nsns.distplot(train[\"SalePrice\"])\nax.set(ylabel = \"Frequency\")\nax.set(xlabel = \"SalePrice\")\nax.set(title = \"SalePrice distribution\")\nsns.despine(trim = True, left = True)  # remove left border from plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:37:40.556839Z","iopub.execute_input":"2023-01-19T14:37:40.557257Z","iopub.status.idle":"2023-01-19T14:37:40.869958Z","shell.execute_reply.started":"2023-01-19T14:37:40.557219Z","shell.execute_reply":"2023-01-19T14:37:40.868766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The distribution looks quite skewed, lets measure its skewness and kurtosis.","metadata":{}},{"cell_type":"code","source":"print(\"Skewness: {}\".format(train[\"SalePrice\"].skew()))\nprint(\"Kurtosis: {}\".format(train[\"SalePrice\"].kurtosis()))","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:37:40.873350Z","iopub.execute_input":"2023-01-19T14:37:40.873843Z","iopub.status.idle":"2023-01-19T14:37:40.882717Z","shell.execute_reply.started":"2023-01-19T14:37:40.873805Z","shell.execute_reply":"2023-01-19T14:37:40.881151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A relatively skewed distribution large tails.","metadata":{"execution":{"iopub.status.busy":"2023-01-14T19:29:57.958127Z","iopub.execute_input":"2023-01-14T19:29:57.959603Z","iopub.status.idle":"2023-01-14T19:29:57.969541Z","shell.execute_reply.started":"2023-01-14T19:29:57.959551Z","shell.execute_reply":"2023-01-14T19:29:57.966837Z"}}},{"cell_type":"markdown","source":"## Features","metadata":{}},{"cell_type":"markdown","source":"Let's plot the features against the target, to see get a good feel for them.","metadata":{}},{"cell_type":"code","source":"def find_numeric_features(features):\n    \"\"\"\n    Find numeric features of a given feature set\n    \n    Args:\n        features (pandas dataframe) : dataframe to inspect\n        \n    Returns:\n        numeric (string[]) : numeric feature names \n    \n    \"\"\"\n    numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    numeric = []\n    for i in features.columns:\n        if features[i].dtype in numeric_dtypes:\n            numeric.append(i)\n    return numeric","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:37:40.884565Z","iopub.execute_input":"2023-01-19T14:37:40.885487Z","iopub.status.idle":"2023-01-19T14:37:40.893663Z","shell.execute_reply.started":"2023-01-19T14:37:40.885440Z","shell.execute_reply":"2023-01-19T14:37:40.892221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_features_against_target(data):\n    \"\"\"\n    Create scatter subplots of all features against target\n    \n    Args:\n        data (pandas dataframe) : data whose features we want to plot\n        \n    Returns:\n        None\n    \"\"\"\n\n    # Finding numeric features\n    numeric = find_numeric_features(data)         \n\n    # define figure\n    fig, axs = plt.subplots(ncols=3, figsize=(12, 120))\n\n    # adjust spacing\n    plt.subplots_adjust(right=2)\n    plt.subplots_adjust(top=2)\n\n    # set colour palette\n    sns.color_palette(\"husl\", 8)\n\n    # for each feature\n    for i, feature in enumerate(list(data[numeric]), 1):\n\n        # plot data\n        plt.subplot(len(list(numeric)), 3, i)\n        sns.scatterplot(x = feature, y='SalePrice', hue='SalePrice', palette='Blues', data = data)\n\n        # add axis labels\n        plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n        plt.ylabel('SalePrice', size=15, labelpad=12.5)\n\n        # change tick sizes\n        plt.tick_params(axis='x', labelsize=16)\n        plt.tick_params(axis='y', labelsize=16)\n\n        # add legend\n        plt.legend(loc='best', prop={'size': 12})\n\n    plt.show()\n    \nplot_features_against_target(train)","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:37:40.895632Z","iopub.execute_input":"2023-01-19T14:37:40.896009Z","iopub.status.idle":"2023-01-19T14:37:54.916743Z","shell.execute_reply.started":"2023-01-19T14:37:40.895975Z","shell.execute_reply":"2023-01-19T14:37:54.915275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that certain features like 'TotalBsmtSF' and 'GrLivArea' are strongly correlated with 'SalePrice'. Other features like 'BsmtFinSF2' have alsmost no correlation. There is also clearly a lot of outliers and corrupt data to remove.","metadata":{}},{"cell_type":"markdown","source":"Let's now plot a correlation heat map to see how features are correlated to each other and to SalePrice.","metadata":{}},{"cell_type":"code","source":"corr = train.corr()\nplt.subplots(figsize = (15, 12))\nsns.heatmap(corr, vmax = 0.9, cmap = 'Blues', square = True)","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:37:54.918220Z","iopub.execute_input":"2023-01-19T14:37:54.918600Z","iopub.status.idle":"2023-01-19T14:37:56.120975Z","shell.execute_reply.started":"2023-01-19T14:37:54.918563Z","shell.execute_reply":"2023-01-19T14:37:56.119727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see some strong correlations here, some more useful than others. For example, \"GarageYrBlt\" being strongly correlated with \"YearBuilt\" is not a surprising relationship.","metadata":{}},{"cell_type":"markdown","source":"Let's look at how some specfic features are correlated with the target.","metadata":{}},{"cell_type":"code","source":"data = pd.concat([train['SalePrice'], train['OverallQual']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=train['OverallQual'], y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:37:56.122534Z","iopub.execute_input":"2023-01-19T14:37:56.122868Z","iopub.status.idle":"2023-01-19T14:37:56.503116Z","shell.execute_reply.started":"2023-01-19T14:37:56.122826Z","shell.execute_reply":"2023-01-19T14:37:56.501925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.concat([train['SalePrice'], train['YearBuilt']], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=train['YearBuilt'], y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=45);\nplt.tick_params(axis='x', labelsize=7)","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:37:56.504492Z","iopub.execute_input":"2023-01-19T14:37:56.505192Z","iopub.status.idle":"2023-01-19T14:38:01.230887Z","shell.execute_reply.started":"2023-01-19T14:37:56.505155Z","shell.execute_reply":"2023-01-19T14:38:01.229798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.concat([train['SalePrice'], train['GrLivArea']], axis=1)\ndata.plot.scatter(x='GrLivArea', y='SalePrice', alpha=0.3, ylim=(0,800000));","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:38:01.234798Z","iopub.execute_input":"2023-01-19T14:38:01.235239Z","iopub.status.idle":"2023-01-19T14:38:01.505263Z","shell.execute_reply.started":"2023-01-19T14:38:01.235197Z","shell.execute_reply":"2023-01-19T14:38:01.504358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"markdown","source":"First we will remove the Ids from the train and test, as they are unique for each row and hence not useful for the model.","metadata":{}},{"cell_type":"code","source":"def drop_ID(X):\n    \"\"\"\n    Drop the ID column since it is unique and so useless for ML\n    \n    Args:\n        X (pandas dataframe) : dataframe whose ID col we want to trop\n    \n    Returns:\n        X (pandas dataframe) : dataframe with ID's dropped\n    \"\"\"\n    X.drop(['Id'], axis = 1, inplace = True)\n    return X\n\ntrain = drop_ID(train)\ntest = drop_ID(test)\ntrain.shape, test.shape","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:38:01.506717Z","iopub.execute_input":"2023-01-19T14:38:01.507077Z","iopub.status.idle":"2023-01-19T14:38:01.521732Z","shell.execute_reply.started":"2023-01-19T14:38:01.507045Z","shell.execute_reply":"2023-01-19T14:38:01.520623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, lets look at the SalePrice distribution again.","metadata":{}},{"cell_type":"code","source":"sns.set_style(\"white\")\nsns.set_color_codes(palette = 'deep')\nfigure, ax = plt.subplots(figsize = (8, 7))\nsns.distplot(train[\"SalePrice\"])\nax.set(ylabel = \"Frequency\")\nax.set(xlabel = \"SalePrice\")\nax.set(title = \"SalePrice distribution\")\nsns.despine(trim = True, left = True)  # remove left border from plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:38:01.523399Z","iopub.execute_input":"2023-01-19T14:38:01.523771Z","iopub.status.idle":"2023-01-19T14:38:01.832430Z","shell.execute_reply.started":"2023-01-19T14:38:01.523737Z","shell.execute_reply":"2023-01-19T14:38:01.831136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Notice that it is skewed to the right. This is bad as models do not work well with skewed data. In order to make it more normal, apply a $\\ln({1+x})$ transform.","metadata":{}},{"cell_type":"code","source":"def transform_target(X):\n    \"\"\"\n    Transform target to make it normally distributed\n    \n    Params:\n        X (pandas dataframe) : dataframe whose target we want to transform\n        \n    Returns:\n        X (pandas dataframe) : transformed dataframe\n    \"\"\"\n    X[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n    return X\n\n# log(1+x) transform\ntrain = transform_target(train)","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:38:01.833791Z","iopub.execute_input":"2023-01-19T14:38:01.834127Z","iopub.status.idle":"2023-01-19T14:38:01.841396Z","shell.execute_reply.started":"2023-01-19T14:38:01.834096Z","shell.execute_reply":"2023-01-19T14:38:01.840203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now replot the distrubition alongside a fitted normal distribution to see how they compare.","metadata":{}},{"cell_type":"code","source":"sns.set_style(\"white\")\nsns.set_color_codes(palette = 'deep')\nfigure, ax = plt.subplots(figsize = (8, 7))\nsns.distplot(train[\"SalePrice\"], fit = norm)\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint('mu = {:.2f}'.format(mu))\nprint('sigma = {:.2f}'.format(sigma))\n\n#Now plot the distribution\nplt.legend([\"Interpolated data\",'Normal fit ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='center left', bbox_to_anchor=(1, 0.5))\nax.set(ylabel = \"Frequency\")\nax.set(xlabel = \"SalePrice\")\nax.set(title = \"SalePrice distribution\")\nsns.despine(trim = True, left = True)  # remove left border from plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:38:01.842758Z","iopub.execute_input":"2023-01-19T14:38:01.843094Z","iopub.status.idle":"2023-01-19T14:38:02.188640Z","shell.execute_reply.started":"2023-01-19T14:38:01.843062Z","shell.execute_reply":"2023-01-19T14:38:02.187463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The target is no longer skewed.","metadata":{}},{"cell_type":"markdown","source":"## Drop outliers","metadata":{}},{"cell_type":"markdown","source":"Using the plots we made before of specific features against the SalePrice, we will remove some of the outliers.","metadata":{}},{"cell_type":"code","source":"def drop_outliers(X):\n    \"\"\"\n    Drop any identified outliers\n    \n    Args:\n        X (pandas dataframe) : dataframe whose outliers we want to drop\n        \n    Returns:\n        X (pandas dataframe) : dataframe with outliers dropped\n    \"\"\"\n    X.drop(X[(X['OverallQual'] < 5) & (X['SalePrice'] > 200000)].index, inplace=True)\n    X.drop(X[(X['GrLivArea'] > 4500) & (X['SalePrice'] < 300000)].index, inplace=True)\n    X.reset_index(drop=True, inplace=True) # restores index after dropping\n    return X\n\ntrain = drop_outliers(train)","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:38:02.190238Z","iopub.execute_input":"2023-01-19T14:38:02.191543Z","iopub.status.idle":"2023-01-19T14:38:02.206058Z","shell.execute_reply.started":"2023-01-19T14:38:02.191495Z","shell.execute_reply":"2023-01-19T14:38:02.204849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare dataset for cleaning","metadata":{}},{"cell_type":"markdown","source":"Now we split the target and features.  \nAlso combine the train and test features so that we can apply the all coming transformation to the entire dataset.","metadata":{}},{"cell_type":"code","source":"def prepare_for_cleaning(train, test):\n    \"\"\"\n    Prepare datasets for cleaning by dropping target and combining into one dataset\n    \n    Args:\n        train (pandas dataframe) : train dataset\n        test (pandas dataframe) : test dataset\n    \n    Returns:\n        y_train (pandas series) : target\n        X (pandas dataframe) : combined data\n    \"\"\"\n    y_train = train['SalePrice'].reset_index(drop=True)\n    X_train = train.drop(['SalePrice'], axis=1) # drop the target\n    X_test = test\n    X = pd.concat([X_train, X_test]).reset_index(drop=True) # combine train and test\n    \n    return y_train, X\n\ny_train, X = prepare_for_cleaning(train, test)\nX.shape","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:38:02.207669Z","iopub.execute_input":"2023-01-19T14:38:02.207981Z","iopub.status.idle":"2023-01-19T14:38:02.238534Z","shell.execute_reply.started":"2023-01-19T14:38:02.207951Z","shell.execute_reply":"2023-01-19T14:38:02.237324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fill missing values","metadata":{}},{"cell_type":"markdown","source":"First, lets print the percentage of missing values from each column.","metadata":{}},{"cell_type":"code","source":"def percent_missing(df):\n    \"\"\"\n    Determine the percentage of missing values in each column of a data frame\n    \n    Args:\n        df (pandas dataframe) : dataframe we want to inspect\n    \n    Returns:\n        dict_x (dict{}) : dictionary mapping column names to percentage values missing in column\n    \"\"\"\n    dict_x = {}\n    for i in range(0, len(df.columns)):\n        dict_x[df.columns[i]] = round(df[df.columns[i]].isnull().mean()*100, 2)\n    return dict_x\n\nmissing = percent_missing(X)\ndf_miss = sorted(missing.items(), key = lambda x : x[1], reverse = True)  # reverse sort\nprint(\"Percentage of missing data\")\nfor i in df_miss:\n    if(i[1] > 0): print(i)","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:38:02.239920Z","iopub.execute_input":"2023-01-19T14:38:02.240239Z","iopub.status.idle":"2023-01-19T14:38:02.281434Z","shell.execute_reply.started":"2023-01-19T14:38:02.240209Z","shell.execute_reply":"2023-01-19T14:38:02.279564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that there are a few columns with lots of missing data.  \nLet's visualise this data.","metadata":{}},{"cell_type":"code","source":"# set up figure with colour formatting\nsns.set_style(\"white\")\nfig, ax = plt.subplots(figsize=(8, 7))\nsns.set_color_codes(palette='deep')\n\n# determine columns with more than one missing value and sort in increasing orber\nmissing = round(X.isnull().mean()*100,2)\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar(color=\"b\")\n\n# tweak the visual presentation\nax.xaxis.grid(False)\nax.set(ylabel=\"Percent of missing values\")\nax.set(xlabel=\"Features\")\nax.set(title=\"Percent missing data by feature\")\nsns.despine(trim=True, left=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:38:02.282929Z","iopub.execute_input":"2023-01-19T14:38:02.283880Z","iopub.status.idle":"2023-01-19T14:38:02.860699Z","shell.execute_reply.started":"2023-01-19T14:38:02.283840Z","shell.execute_reply":"2023-01-19T14:38:02.859227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we want to impute missing values for each of these features.","metadata":{}},{"cell_type":"code","source":"def handle_missing(features):\n    # the data description states that NA refers to typical ('Typ') values\n    features['Functional'] = features['Functional'].fillna('Typ')\n    \n    # Replace the missing values in each of the columns below with their mode\n    features['Electrical'] = features['Electrical'].fillna(features['Electrical'].mode())\n    features['KitchenQual'] = features['KitchenQual'].fillna(features['KitchenQual'].mode())\n    features['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode())\n    features['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode())\n    features['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode())\n    \n    # zoning is likely based on subclass so we fill based on the mode in that zone\n    features['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n    \n    # the data description stats that NA refers to \"No Pool\"\n    features[\"PoolQC\"] = features[\"PoolQC\"].fillna(\"None\")\n    # Replacing the missing values with 0, since no garage = no cars in garage\n    for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n        features[col] = features[col].fillna(0)\n    # Replacing the missing values with None\n    for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n        features[col] = features[col].fillna('None')\n    # NaN values for these categorical basement features, means there's no basement\n    for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n        features[col] = features[col].fillna('None')\n        \n    # lot frontage likely based on neighbourhood so fill based on median in neighbourhood\n    features['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n    \n    # We have no particular intuition around how to fill in the rest of the categorical features\n    # So we replace their missing values with None\n    objects = []\n    for i in features.columns:\n        if features[i].dtype == object:\n            objects.append(i)\n    features.update(features[objects].fillna('None'))\n    \n    # And we do the same thing for numerical features, but this time with 0s\n    numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    numeric = []\n    for i in features.columns:\n        if features[i].dtype in numeric_dtypes:\n            numeric.append(i)\n    features.update(features[numeric].fillna(0))    \n    return features\n    \nX = handle_missing(X)","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:38:02.863810Z","iopub.execute_input":"2023-01-19T14:38:02.864958Z","iopub.status.idle":"2023-01-19T14:38:02.989101Z","shell.execute_reply.started":"2023-01-19T14:38:02.864903Z","shell.execute_reply":"2023-01-19T14:38:02.987967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's now check that we have handled all the missing features.","metadata":{}},{"cell_type":"code","source":"missing = percent_missing(X)\ndf_miss = sorted(missing.items(), key = lambda x : x[1], reverse = True)  # reverse sort\nprint(\"Percentage of missing data\")\nfor i in df_miss:\n    if(i[1] > 0): print(i)","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:38:02.990385Z","iopub.execute_input":"2023-01-19T14:38:02.991056Z","iopub.status.idle":"2023-01-19T14:38:03.028784Z","shell.execute_reply.started":"2023-01-19T14:38:02.991023Z","shell.execute_reply":"2023-01-19T14:38:03.027452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As desired, there are no missing values anymore.","metadata":{}},{"cell_type":"markdown","source":"## Fix skewed features","metadata":{}},{"cell_type":"markdown","source":"Models struggle to deal with non normally distributed features, so we want to transform these in some way.  \nLets plot the numeric features.","metadata":{}},{"cell_type":"code","source":"def plot_numeric_feature_distributions(features):\n    \"\"\"\n    Create boxplot distribution of numeric features\n    \n    Args:\n        features (pandas dataframe) : dataframe to inspect\n        \n    Returns:\n        None\n    \"\"\"\n\n    # set up figure\n    sns.set_style(\"white\")\n    fig, ax = plt.subplots(figsize=(8, 7))\n    ax.set_xscale(\"log\")\n\n    # find numeric features and plot\n    numeric = find_numeric_features(features)\n    ax = sns.boxplot(data = features[numeric] , orient=\"h\", palette=\"Set1\")\n\n    # edit figure\n    ax.xaxis.grid(False)\n    ax.set(ylabel=\"Feature names\")\n    ax.set(xlabel=\"Numeric values\")\n    ax.set(title=\"Numeric Distribution of Features\")\n    sns.despine(trim=True, left=True)\n    \nplot_numeric_feature_distributions(X)","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:38:03.030447Z","iopub.execute_input":"2023-01-19T14:38:03.031523Z","iopub.status.idle":"2023-01-19T14:38:04.656341Z","shell.execute_reply.started":"2023-01-19T14:38:03.031477Z","shell.execute_reply":"2023-01-19T14:38:04.655320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's set a cutoff of skew = 0.5 and find features with a high skewness.","metadata":{}},{"cell_type":"code","source":"def find_skewed_features(features, cutoff):\n    \"\"\"\n    Determine all features which are skewed above some threshold\n    \n    Args:\n        features (pandas dataframe) : dataframe to inspect\n        cutoff (int) : minimum skew\n        \n    Returns\n        skew_features (pandas series) : skewness of each numeric feature\n        high_skew_index (pandas index) : names of high skew features\n    \n    \"\"\"\n    \n    numeric = find_numeric_features(features) # get numeric features\n    skew_features = features[numeric].apply(lambda x : skew(x)).sort_values(ascending = False) # find skewed features and sort\n    high_skew = skew_features[skew_features > skew_cutoff] # filter by skew cutoff\n    high_skew_index = high_skew.index\n    return skew_features, high_skew_index","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:38:04.657649Z","iopub.execute_input":"2023-01-19T14:38:04.657973Z","iopub.status.idle":"2023-01-19T14:38:04.665315Z","shell.execute_reply.started":"2023-01-19T14:38:04.657942Z","shell.execute_reply":"2023-01-19T14:38:04.663700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skew_cutoff = 0.5\nskew_features, high_skew_index = find_skewed_features(X, skew_cutoff)\n\n# print results\nprint(\"There are {} numerical features with a skew > {}:\".format(skew_features.shape[0], skew_cutoff))\nskew_features.head(10)","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:38:04.666612Z","iopub.execute_input":"2023-01-19T14:38:04.667039Z","iopub.status.idle":"2023-01-19T14:38:04.699199Z","shell.execute_reply.started":"2023-01-19T14:38:04.667005Z","shell.execute_reply":"2023-01-19T14:38:04.698270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can use boxcox1 function to compute the Box-Cox transformation.","metadata":{}},{"cell_type":"code","source":"def fix_skewed_features(X):\n    \"\"\"\n    Find then fix skewed features\n    \n    Args:\n        X (pandas dataframe) : dataframe whose skewed features we want to fix\n        \n    Returns\n        X (pandas dataframe) : dataframe with skewed features fixed\n    \"\"\"\n    \n    # find all skewed features\n    kew_cutoff = 0.5\n    skew_features, high_skew_index = find_skewed_features(X, skew_cutoff)\n    \n    # transform features\n    for i in high_skew_index:\n        X[i] = boxcox1p(X[i], boxcox_normmax(X[i] + 1))\n        \n    return X\n\nX = fix_skewed_features(X)","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:38:04.700690Z","iopub.execute_input":"2023-01-19T14:38:04.701676Z","iopub.status.idle":"2023-01-19T14:38:04.924047Z","shell.execute_reply.started":"2023-01-19T14:38:04.701630Z","shell.execute_reply":"2023-01-19T14:38:04.922906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's plot the numeric features again and see if we have corrected the skewness.","metadata":{}},{"cell_type":"code","source":"plot_numeric_feature_distributions(X)","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:38:04.926182Z","iopub.execute_input":"2023-01-19T14:38:04.926646Z","iopub.status.idle":"2023-01-19T14:38:06.296355Z","shell.execute_reply.started":"2023-01-19T14:38:04.926600Z","shell.execute_reply":"2023-01-19T14:38:06.295185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The features are much less skewed now.","metadata":{}},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"## Creating features","metadata":{}},{"cell_type":"markdown","source":"ML models have trouble with complex features, so we will create some simpler ones using intution.","metadata":{}},{"cell_type":"code","source":"def create_new_features(X):\n    \"\"\"\n    Create new features for the data\n    \n    Args:\n        X (pandas dataframe) : dataframe which we want to determine new features for\n    \n    Returns:\n        X (pandas dataframe) : dataframe with new features    \n    \"\"\"\n\n    # features that determine whether the property possesses something\n    #X['BsmtFinType1_Unf'] = (X['BsmtFinType1'] == 'Unf') * 1\n    X['HasWoodDeck'] = (X['WoodDeckSF'] == 0) * 1\n    X['HasOpenPorch'] = (X['OpenPorchSF'] == 0) * 1\n    X['HasEnclosedPorch'] = (X['EnclosedPorch'] == 0) * 1\n    X['Has3SsnPorch'] = (X['3SsnPorch'] == 0) * 1\n    X['HasScreenPorch'] = (X['ScreenPorch'] == 0) * 1\n    X['HasPool'] = (X['PoolArea'] == 0) * 1\n    X['Has2ndFloor'] = (X['2ndFlrSF'] == 0) * 1\n    X['HasGarage'] = (X['GarageArea'] == 0) * 1\n    X['HasBsmt'] = (X['TotalBsmtSF'] == 0) * 1\n    X['HasFireplace'] = (X['Fireplaces'] == 0) * 1\n\n    # add some features together\n    X['Total_Home_Quality'] = X['OverallQual'] + X['OverallCond']\n    X['TotalSF'] = X['TotalBsmtSF'] + X['1stFlrSF'] + X['2ndFlrSF']\n    X['Total_sqr_footage'] = (X['BsmtFinSF1'] + X['BsmtFinSF2'] +\n                                     X['1stFlrSF'] + X['2ndFlrSF'])\n    X['Total_Bathrooms'] = (X['FullBath'] + (0.5 * X['HalfBath']) +\n                                   X['BsmtFullBath'] + (0.5 * X['BsmtHalfBath']))\n    X['Total_porch_sf'] = (X['OpenPorchSF'] + X['3SsnPorch'] +\n                                  X['EnclosedPorch'] + X['ScreenPorch'] +\n                                  X['WoodDeckSF'])\n\n    # other interesting things\n    X['YearsSinceRemodel'] = X['YrSold'].astype(int) - X['YearRemodAdd'].astype(int)\n    X['YrBltAndRemod'] = X['YearBuilt'] + X['YearRemodAdd']\n    \n    return X\n\nX = create_new_features(X)","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:38:06.297949Z","iopub.execute_input":"2023-01-19T14:38:06.298306Z","iopub.status.idle":"2023-01-19T14:38:06.325498Z","shell.execute_reply.started":"2023-01-19T14:38:06.298272Z","shell.execute_reply":"2023-01-19T14:38:06.324399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature transformations","metadata":{}},{"cell_type":"markdown","source":"ML models struggle to tell if log(feature) or (feature)^2 is a good predictor of the target, so we will manually add these.","metadata":{}},{"cell_type":"code","source":"def log_transform(features, log_features):\n    \"\"\"\n    Use log transform on certain features and store them in the dataframe\n    \n    Args:\n        features (pandas dataframe) : dataframe where new features will be stored\n        log_features (string[]) : names of features to log transform\n        \n    Returns:\n        features (pandas dataframe) : transformed dataframe\n    \"\"\"\n    for log_feature in log_features:\n        features[log_feature + '_log'] = np.log(1.01 + features[log_feature])\n    return features\n\n# get all features to log transform\nlog_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n                'TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n                'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n                'TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF',\n                'EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','YearRemodAdd','TotalSF']\n\n# apply log transform\nX = log_transform(X, log_features)","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:38:06.326885Z","iopub.execute_input":"2023-01-19T14:38:06.327240Z","iopub.status.idle":"2023-01-19T14:38:06.357895Z","shell.execute_reply.started":"2023-01-19T14:38:06.327206Z","shell.execute_reply":"2023-01-19T14:38:06.356768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def square_transform(features, squ_features):\n    \"\"\"\n    Use square transform on certain features and store them in the dataframe\n    \n    Args:\n        features (pandas dataframe) : dataframe where new features will be stored\n        squ_features (string[]) : names of features to square transform\n        \n    Returns:\n        features (pandas dataframe) : transformed dataframe\n    \"\"\"\n    for squ_feature in squ_features:\n        features[squ_feature + \"_squ\"] = features[squ_feature] * features[squ_feature]\n    return features\n\n# get all features to square transform\nsquared_features = ['YearRemodAdd', 'LotFrontage_log', \n                    'TotalBsmtSF_log', '1stFlrSF_log', '2ndFlrSF_log', 'GrLivArea_log',\n                    'GarageCars_log', 'GarageArea_log']\n\n# apply square transform\nX = square_transform(X, squared_features)","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:38:06.364851Z","iopub.execute_input":"2023-01-19T14:38:06.365212Z","iopub.status.idle":"2023-01-19T14:38:06.379328Z","shell.execute_reply.started":"2023-01-19T14:38:06.365180Z","shell.execute_reply":"2023-01-19T14:38:06.378279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Encode categorical features","metadata":{}},{"cell_type":"markdown","source":"Most models can only handle numerical features so we will convert our categorical features.","metadata":{}},{"cell_type":"code","source":"def encode_categorical(X):\n    \"\"\"\n    Encode caletgorical features\n    \n    Args:\n        X (pandas dataframe) : dataframe whose categorical features we want to encode\n        \n    Returns:\n        X (pandas dataframe) : dataframe with categorical features encoded\n    \"\"\"\n    X = pd.get_dummies(X).reset_index(drop = True)\n    return X\n\nX = encode_categorical(X)\nX.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:38:06.380851Z","iopub.execute_input":"2023-01-19T14:38:06.382078Z","iopub.status.idle":"2023-01-19T14:38:06.617614Z","shell.execute_reply.started":"2023-01-19T14:38:06.382032Z","shell.execute_reply":"2023-01-19T14:38:06.616604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Recreate training and test sets","metadata":{}},{"cell_type":"code","source":"def extract_train_and_test(X, target):\n    \"\"\"\n    Extract the train and test sets from the combined data\n    \n    Args:\n        X (pandas dataframe) : dataframe to split up\n        target (pandas series) : target data\n        \n    Return:\n        X_test (pandas dataframe) : test data\n        X_train (pandas dataframe) : train data\n    \"\"\"\n    X_train = X.iloc[:len(y_train), :]\n    X_test = X.iloc[len(y_train):, :]\n    return X_train, X_test\n\nX_train, X_test = extract_train_and_test(X, y_train)\nX_train.shape, X_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:38:06.618856Z","iopub.execute_input":"2023-01-19T14:38:06.619842Z","iopub.status.idle":"2023-01-19T14:38:06.628339Z","shell.execute_reply.started":"2023-01-19T14:38:06.619806Z","shell.execute_reply":"2023-01-19T14:38:06.627151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:38:06.630049Z","iopub.execute_input":"2023-01-19T14:38:06.630563Z","iopub.status.idle":"2023-01-19T14:38:06.809400Z","shell.execute_reply.started":"2023-01-19T14:38:06.630522Z","shell.execute_reply":"2023-01-19T14:38:06.808391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data pipeline","metadata":{}},{"cell_type":"markdown","source":"We know want to combine all of the above steps into a single pipeline function.","metadata":{}},{"cell_type":"code","source":"def data_pipeline():\n    \"\"\"\n    Data pipeline to read, clean and engineer features\n    \n    Args:\n        None\n        \n    Returns:\n        X_train (pandas dataframe) : train data\n        X_test (pandas dataframe) : test data\n        y_train (pandas series) : target\n    \"\"\"\n    # read in data\n    train, test = read_data()\n\n    # drop id column\n    train = drop_ID(train)\n    test = drop_ID(test)\n    \n    # transform the target to normalise\n    #train = transform_target(train)\n    train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n    # drop outliers\n    train = drop_outliers(train)\n\n    # extract target and combine train and test for cleaning\n    y_train, X = prepare_for_cleaning(train, test)\n\n    # fill all missing values\n    X = handle_missing(X)\n\n    # normalise skewed features\n    X = fix_skewed_features(X)\n\n    # create new features\n    X = create_new_features(X)\n\n    # log transform \n    log_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n                    'TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n                    'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n                    'TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF',\n                    'EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','YearRemodAdd','TotalSF']\n    X = log_transform(X, log_features)\n\n    # square transform\n    squared_features = ['YearRemodAdd', 'LotFrontage_log', \n                        'TotalBsmtSF_log', '1stFlrSF_log', '2ndFlrSF_log', 'GrLivArea_log',\n                        'GarageCars_log', 'GarageArea_log']\n    X = square_transform(X, squared_features)\n\n    # encode categorical features\n    X = encode_categorical(X)\n\n    # decouple the train and test data\n    X_train, X_test = extract_train_and_test(X, y_train)\n\n    return X_train, X_test, y_train\n\nX_train, X_test, y_train = data_pipeline()\nX_train.shape, X_test.shape, y_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:38:06.812937Z","iopub.execute_input":"2023-01-19T14:38:06.813267Z","iopub.status.idle":"2023-01-19T14:38:07.286103Z","shell.execute_reply.started":"2023-01-19T14:38:06.813239Z","shell.execute_reply":"2023-01-19T14:38:07.285121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.isnull().values.any(), X_test.isnull().values.any(), y_train.isnull().values.any()","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:38:07.287564Z","iopub.execute_input":"2023-01-19T14:38:07.287873Z","iopub.status.idle":"2023-01-19T14:38:07.297470Z","shell.execute_reply.started":"2023-01-19T14:38:07.287843Z","shell.execute_reply":"2023-01-19T14:38:07.296197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Building a model","metadata":{}},{"cell_type":"markdown","source":"## Key features of training a model:\n- **Cross Validation** Using 12-fold cross-validation\n- **Models:** On each run of cross-validation I fit 7 models (ridge, svr, gradient boosting, random forest, xgboost, lightgbm regressors)\n- **Stacking:** In addition, I trained a meta StackingCVRegressor optimized using xgboost\n- **Blending:** All models trained will overfit the training data to varying degrees. Therefore, to make final predictions, I blended their predictions together to get more robust predictions.","metadata":{}},{"cell_type":"markdown","source":"## Setup cross validation and define error metrics","metadata":{}},{"cell_type":"markdown","source":"We will use a 12 fold validation.","metadata":{"execution":{"iopub.status.busy":"2023-01-16T20:30:44.607061Z","iopub.execute_input":"2023-01-16T20:30:44.607482Z","iopub.status.idle":"2023-01-16T20:30:44.614441Z","shell.execute_reply.started":"2023-01-16T20:30:44.607444Z","shell.execute_reply":"2023-01-16T20:30:44.613286Z"}}},{"cell_type":"code","source":"kf = KFold(n_splits = 12, random_state = 42, shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:38:07.299110Z","iopub.execute_input":"2023-01-19T14:38:07.299457Z","iopub.status.idle":"2023-01-19T14:38:07.309014Z","shell.execute_reply.started":"2023-01-19T14:38:07.299426Z","shell.execute_reply":"2023-01-19T14:38:07.307873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The error metric we use is the root mean squared log error (rmsle).","metadata":{}},{"cell_type":"code","source":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(np.log(y), np.log(y_pred)))\n\ndef cv_rmse(model, X, y):\n    score = cross_val_score(\n        model, X, y, cv=kf, scoring=\"neg_mean_squared_error\",\n    )\n    score = np.sqrt(-score)\n    score_mean = score.mean()\n    score_std = score.std()\n    return score_mean, score_std","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:38:07.310566Z","iopub.execute_input":"2023-01-19T14:38:07.310941Z","iopub.status.idle":"2023-01-19T14:38:07.321403Z","shell.execute_reply.started":"2023-01-19T14:38:07.310909Z","shell.execute_reply":"2023-01-19T14:38:07.320082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setup models","metadata":{}},{"cell_type":"markdown","source":"Let's setup the models now. To begin with we will keep all hyperparameters to their default values. These can be tweaked later.","metadata":{}},{"cell_type":"code","source":"# Light Gradient Boosting Regressor\nlightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=4,\n                       learning_rate=0.01, \n                       n_estimators=5000,\n                       max_bin=200, \n                       bagging_fraction=0.75,\n                       bagging_freq=5, \n                       bagging_seed=7,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=7,\n                       verbose=-1)\n\n# XGBoost Regressor\nxgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=3460,\n                       max_depth=3, \n                       min_child_weight=0,\n                       gamma=0, \n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:squarederror', \n                       nthread=-1,\n                       scale_pos_weight=1, \n                       seed=27,\n                       reg_alpha=0.00006)\n\n# Support Vector Regressor\nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003,))\n\n# Ridge Regressor\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kf))\n\n# Lasso\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kf))\n\n# Elasticnet\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004,0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kf, l1_ratio=e_l1ratio))   \n\n# Gradient Boosting Regressor\ngbr = GradientBoostingRegressor(n_estimators=3000, \n                                learning_rate=0.05, \n                                max_depth=4, \n                                max_features='sqrt', \n                                min_samples_leaf=15, \n                                min_samples_split=10, \n                                loss='huber', \n                                random_state =42)  \n\n# Random Forest Regressor\nrf = RandomForestRegressor()\n\n# Stack up all the models above, optimized using xgboost\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr, xgboost, lightgbm),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:38:07.322894Z","iopub.execute_input":"2023-01-19T14:38:07.323512Z","iopub.status.idle":"2023-01-19T14:38:07.337833Z","shell.execute_reply.started":"2023-01-19T14:38:07.323478Z","shell.execute_reply":"2023-01-19T14:38:07.336267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get cross validation scores","metadata":{}},{"cell_type":"code","source":"def score_models_individually(X_train, y_train):\n    scores = {}\n\n    # lightgbm\n    score_mean, score_std = cv_rmse(lightgbm, X_train, y_train)\n    print(\"lightgbm: {:.4f} ({:.4f})\".format(score_mean, score_std))\n    scores['lgb'] = (score_mean, score_std)\n\n    # xgboost\n    score_mean, score_std = cv_rmse(xgboost, X_train, y_train)\n    print(\"xgboost: {:.4f} ({:.4f})\".format(score_mean, score_std))\n    scores['xgb'] = (score_mean, score_std)\n    \n    # svr\n    score_mean, score_std = cv_rmse(svr, X_train, y_train)\n    print(\"SVR: {:.4f} ({:.4f})\".format(score_mean, score_std))\n    scores['svr'] = (score_mean, score_std)\n\n    # ridge\n    score_mean, score_std = cv_rmse(ridge, X_train, y_train)\n    print(\"ridge: {:.4f} ({:.4f})\".format(score_mean, score_std))\n    scores['ridge'] = (score_mean, score_std)\n    \n    # lasso\n    score_mean, score_std = cv_rmse(lasso, X_train, y_train)\n    print(\"lasso: {:.4f} ({:.4f})\".format(score_mean, score_std))\n    scores['ridge'] = (score_mean, score_std)\n    \n    # elasticnet\n    score_mean, score_std = cv_rmse(elasticnet, X_train, y_train)\n    print(\"elasticnet: {:.4f} ({:.4f})\".format(score_mean, score_std))\n    scores['ridge'] = (score_mean, score_std)\n    \n    # gbr\n    score_mean, score_std = cv_rmse(gbr, X_train, y_train)\n    print(\"gbr: {:.4f} ({:.4f})\".format(score_mean, score_std))\n    scores['gbr'] = (score_mean, score_std)\n\n    # rf\n    score_mean, score_std = cv_rmse(rf, X_train, y_train)\n    print(\"rf: {:.4f} ({:.4f})\".format(score_mean, score_std))\n    scores['rf'] = (score_mean, score_std)\n    \n    return scores\n\nscores = score_models_individually(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:38:07.339514Z","iopub.execute_input":"2023-01-19T14:38:07.339907Z","iopub.status.idle":"2023-01-19T14:59:09.567925Z","shell.execute_reply.started":"2023-01-19T14:38:07.339872Z","shell.execute_reply":"2023-01-19T14:59:09.566708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fit the models","metadata":{}},{"cell_type":"markdown","source":"Now we fit all the models on the trainning data.","metadata":{}},{"cell_type":"code","source":"print('stack_gen')\nstack_gen_model = stack_gen.fit(np.array(X_train), np.array(y_train))\n\nprint('lightgbm')\nlgb_model_full_data = lightgbm.fit(X_train, y_train)\n\nprint('xgboost')\nxgb_model_full_data = xgboost.fit(X_train, y_train)\n\nprint('Svr')\nsvr_model_full_data = svr.fit(X_train, y_train)\n\nprint('Ridge')\nridge_model_full_data = ridge.fit(X_train, y_train)\n\nprint('elasticnet')\nelastic_model_full_data = elasticnet.fit(X_train, y_train)\n\nprint('Lasso')\nlasso_model_full_data = lasso.fit(X_train, y_train)\n\nprint('GradientBoosting')\ngbr_model_full_data = gbr.fit(X_train, y_train)\n\nprint('RandomForest')\nrf_model_full_data = rf.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-01-19T14:59:09.569398Z","iopub.execute_input":"2023-01-19T14:59:09.569773Z","iopub.status.idle":"2023-01-19T15:11:39.796057Z","shell.execute_reply.started":"2023-01-19T14:59:09.569741Z","shell.execute_reply":"2023-01-19T15:11:39.794897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Blend models and get predictions","metadata":{}},{"cell_type":"markdown","source":"Finally we blend all of the models which make the predictions more robust to overfitting.","metadata":{}},{"cell_type":"code","source":"def blend_models_predict(X):\n    return ((0.15 * elastic_model_full_data.predict(X)) + \\\n            (0.15 * lasso_model_full_data.predict(X)) + \\\n            (0.05 * rf_model_full_data.predict(X)) + \\\n            (0.15 * ridge_model_full_data.predict(X)) + \\\n            (0.05 * svr_model_full_data.predict(X)) + \\\n            (0.05 * gbr_model_full_data.predict(X)) + \\\n            (0.05 * xgb_model_full_data.predict(X)) + \\\n            (0.05 * lgb_model_full_data.predict(X)) + \\\n            (0.3 * stack_gen_model.predict(np.array(X))))\n\n# Get final precitions from the blended model\nblended_score = rmsle(y_train, blend_models_predict(X_train))\nscores['blended'] = (blended_score, 0)\nprint('RMSLE score on train data:')\nprint(blended_score)","metadata":{"execution":{"iopub.status.busy":"2023-01-19T15:21:20.727879Z","iopub.execute_input":"2023-01-19T15:21:20.728256Z","iopub.status.idle":"2023-01-19T15:21:22.649531Z","shell.execute_reply.started":"2023-01-19T15:21:20.728225Z","shell.execute_reply":"2023-01-19T15:21:22.648475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's plot the scores of each of our models to see how they compare.","metadata":{}},{"cell_type":"code","source":"# set up figure\nsns.set_style(\"white\")\nfig = plt.figure(figsize=(24, 12))\n\n# plot points\nax = sns.pointplot(x=list(scores.keys()), y=[score for score, _ in scores.values()], markers=['o'], linestyles=['-'])\nfor i, score in enumerate(scores.values()):\n    ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n\n# edit graph\nplt.ylabel('Score (RMSE)', size=20, labelpad=12.5)\nplt.xlabel('Model', size=20, labelpad=12.5)\nplt.tick_params(axis='x', labelsize=13.5)\nplt.tick_params(axis='y', labelsize=12.5)\nplt.title('Scores of Models', size=20)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-19T15:11:41.246255Z","iopub.execute_input":"2023-01-19T15:11:41.246628Z","iopub.status.idle":"2023-01-19T15:11:41.590991Z","shell.execute_reply.started":"2023-01-19T15:11:41.246596Z","shell.execute_reply":"2023-01-19T15:11:41.589680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submitting final predictions","metadata":{}},{"cell_type":"code","source":"predictions = np.floor(np.expm1(blend_models_predict(X_test))) # exponentialise because the predictions are logged\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\noutput = pd.DataFrame({'Id': test[\"Id\"], 'SalePrice': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","metadata":{"execution":{"iopub.status.busy":"2023-01-19T15:21:36.227132Z","iopub.execute_input":"2023-01-19T15:21:36.227616Z","iopub.status.idle":"2023-01-19T15:21:37.870687Z","shell.execute_reply.started":"2023-01-19T15:21:36.227580Z","shell.execute_reply":"2023-01-19T15:21:37.869394Z"},"trusted":true},"execution_count":null,"outputs":[]}]}